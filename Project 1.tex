\documentclass[conference]{IEEEtran}

\usepackage{graphicx}   
\usepackage{amsmath}    
\usepackage{booktabs}   
\usepackage{url}      

\title{Predicting Critical Temperature of Superconductors: Lasso vs Ridge Regression}

\author{
  \IEEEauthorblockN{Miguel Ramirez}
  \IEEEauthorblockA{
    Department of Statistics and Data Science \\
    University of Central Florida \\
    Orlando, United States \\
    mi723380@ucf.edu}
}

\begin{document}

\maketitle

\begin{abstract}
This project uses a LASSO and Ridge regression model to predict the superconducting critical temperature ($T_c$). 
The dataset is provided by the UCI Machine Learning Repository and contains 21,263 superconductors with 81 predictor variables. 
Both models are evaluated using cross-validation, with performance measured by Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and $R^2$.
\end{abstract}
\begin{IEEEkeywords}
Superconductor Critical Temperature, UCI Dataset, LASSO Regression, Ridge Regression
\end{IEEEkeywords}

\section*{Introduction}

The critical temperature ($T_c$) of a superconducting material is the specific temperature below which it exhibits zero electrical resistance and transitions from a normal conducting state to a superconducting state \cite{sciencedirect_tc}.
According to the U.S. Department of Energy, superconductivity is important because it enables electricity to flow without energy loss and allows materials to repel magnetic fields \cite{doe_superconductivity}.
These properties are the foundation for technologies such as MRI machines, particle accelerators, and quantum computers. 
Furthermore, advances in high-temperature superconductors could have transformative applications in energy-efficient power transmission.

\section{Data}
The Superconductivity Data Set from the UCI Machine Learning Repository provides information on 21,263 superconductors with 81 predictor variables and no missing values. 
Each entry represents a superconductor with various measurements and statistics of said superconductor. These include \emph{number\_of\_elements}, \emph{mean\_atomic\_mass}, \emph{mean\_field\_strength} and other various measurements/statistics. 
The target variable is the critical temperature ($T_c$) at which the superconductor becomes superconducting[2]. 

\section {Data analysis}

Table~\ref{tab:summary} provides the five-number summary and the standard deviation 
of the critical temperature ($T_c$). As there are 81 predictors in this data set, I could not show a correlation plot to display the multicollinearity.
So I decided to show the top 10 most correlated pairs as shown in Table~\ref{tab:top_corr}.
Figure~\ref{fig:tc_distribution} illustrates the distribution of Critical Temperature ($T_c$). The values indicate that the distribution is 
right-skewed. The five number summary confirms this as well.

So ordinary OLS regression is not suitable for this dataset due to the risk of overfitting.
Additionally, the 81 predictor variables create a high-dimensional setting as shown in Table~\ref{tab:top_corr}.

\begin{table}[h]
\centering
\caption{Five-Number Summary and Standard Deviation of Critical Temperature $T_c$ }
\label{tab:summary}
\begin{tabular}{ccccccc}
\toprule
Min & 1st Qu. & Median & Mean & 3rd Qu. & Max & Std \\\\
\midrule
0.0002 & 5.365 & 20 & 34.42 & 63 & 185 & 34.25 \\\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\centering
\caption{Top 10 Most Correlated Predictor Pairs ($|r|$)}
\label{tab:top_corr}
\begin{tabular}{l l c}
\toprule
Variable 1 & Variable 2 & $|r|$ \\
\midrule
pred\_lasso & pred\_ridge & 1.000 \\
resid\_lasso & resid\_ridge & 0.999 \\
entropy\_fie & entropy\_atomic\_radius & 0.998 \\
wtd\_mean\_Valence & wtd\_gmean\_Valence & 0.995 \\
entropy\_fie & entropy\_Valence & 0.993 \\
wtd\_mean\_fie & wtd\_gmean\_fie & 0.992 \\
mean\_Valence & gmean\_Valence & 0.990 \\
entropy\_atomic\_radius & entropy\_Valence & 0.990 \\
range\_ThermalConductivity & std\_ThermalConductivity & 0.988 \\
range\_FusionHeat & std\_FusionHeat & 0.985 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{temp_distr.png}
    \caption{Distribution of ritical temperature ($T_c$)}
    \label{fig:tc_distribution}
\end{figure}

\section{Methodology}
\noindent In general a regression model can be expressed as:
\[
Y = \beta_0 + \beta_1X_1 + \cdots + \beta_pX_p + \epsilon
\]
where $Y$ is the critical Temperature $T_c$ and $X_i$ are the predictor variables.

Due to the amount of possible features (81), I'll employ two types of regression models that can help with feature selection.
\subsection{Ridge Regression}
Ridge regression is a regularized form of linear regression that adds an $L_2$ penalty (squared coefficients) to the ordinary least squares objective function:

\[
\min_{\beta} \left\{ \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 
+ \lambda \sum_{j=1}^{p} \beta_j^2 \right\}.
\]

where:
\begin{itemize}
    \item $y_i$ = observed response for sample $i$,
    \item $x_{ij}$ = value of predictor $j$ for sample $i$,
    \item $\beta_j$ = coefficient for predictor $j$,
    \item $p$ = number of predictors,
    \item $\lambda \geq 0$ = regularization parameter controlling penalty strength.
\end{itemize}

The $L_2$ penalty shrinks coefficients toward zero but does not set them exactly to zero. This helps mitigate multicollinearity and overfitting.

\subsection{LASSO Regression}
LASSO (Least Absolute Shrinkage and Selection Operator) uses the same framework as Ridge regression, but it applies an $L_1$ penalty (absolute value of coefficients) instead of $L_2$:

\[
\min_{\beta} \left\{ \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij}\right)^2 
+ \lambda \sum_{j=1}^{p} |\beta_j| \right\}.
\]

where the terms are the same as above, except:
\begin{itemize}
    \item $\sum_{j=1}^{p} |\beta_j|$ = the $L_1$ penalty
\end{itemize}

The $L_1$ penalty not only shrinks coefficients but can also set some of them exactly to zero. This allows LASSO to perform variable selection automatically, keeping only the most relevant predictors in the model.

\subsection{Model Comparison}
To evaluate and compare the performance of LASSO and Ridge regression, 
I used 10-fold cross-validation on the training data. 
In this setup, the dataset was randomly partitioned into ten equally sized folds. 

The optimal regularization parameter $\lambda$ for both LASSO ($\alpha=1$) 
and Ridge ($\alpha=0$) regression were chosen automatically by minimizing the 
cross-validated mean squared error using the \texttt{cv.glmnet} function in R. 
For Ridge, only predictors with coefficients satisfying $|\beta_j| \geq 0.05$. This was an arbitrary cut off. 
I tried .01 as the cut off and It kept all 81 predictors.

Table~\ref{tab:cv_compare} reports the averaged metrics over all folds, 
allowing us to directly compare the predictive accuracy of the two models.

\begin{table}[htbp]
\centering
\caption{10-Fold Cross-Validation: LASSO vs Ridge}
\label{tab:cv_compare}
\begin{tabular}{lccc}
\toprule
Model & RMSE & MAE & $R^2$ \\
\midrule
LASSO  & 17.6705 & 13.3733 & 0.7340 \\
Ridge ($\geq$ 0.05) & 18.9524 & 14.6088 & 0.6951 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{predict_residuals.png}
    \caption{Actual vs Predicted critical temperature ($T_c$) for LASSO and Ridge. 
    The 45-degree line represents perfect prediction.}
    \label{fig:actual_vs_pred}
\end{figure}

Figure~\ref{fig:actual_vs_pred} shows the predicted versus actual critical temperatures. 
The graphs are similar for both models. Which The RMSE, MAE and $R^2$ values in Table~\ref{tab:cv_compare} confirm.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{qq_plots.png}
    \caption{Q--Q plots of residuals for LASSO (left) and Ridge (right). 
    Deviations from the red line indicate departures from normality.}
    \label{fig:qqplots}
\end{figure}

Figure~\ref{fig:qqplots} shows the Qâ€“Q plots for residuals. Both LASSO and Ridge display deviations from the reference line in the tails, suggesting that residuals are not normally distributed.
\section{Conclusion}
In this project I compared LASSO and Ridge regression for predicting the 
critical temperature ($T_c$) of superconductors. Since the dataset contains 81 
predictors that showed multicollinearity, a regularized approach was needed 
instead of ordinary least squares. 

The results from 10-fold cross-validation showed that LASSO gave better 
predictions than Ridge, with lower error and higher $R^2$. But only slightly. 
Lasso ended up using 78 predictors and Ridge used 78 predictors as well. 

If I were to improve on this project, 
I would consider trying an elastic net approach, which blends LASSO and Ridge. 
Or try different models such as Random Forest or Gradient Boosting Machines.

\appendix


\section{R Code}
\url{https://github.com/MiguelARam27/data-mining-project-1}

\begin{thebibliography}{00}

\bibitem{sciencedirect_tc}
Superconducting Critical Temperature \emph{ScienceDirect Topics}, Elsevier. [Online]. 
Available: https://www.sciencedirect.com/topics/chemistry/superconducting-critical-temperature. 

\bibitem{uci_dataset}
D. Dua and C. Graff, ``UCI Machine Learning Repository: Superconductivity Data Set,'' 
\emph{University of California, Irvine, School of Information and Computer Sciences}, 2018. 
[Online]. Available: https://archive.ics.uci.edu/dataset/464/superconductivty+data. 
[Accessed: Sept. 25, 2025].

\bibitem{doe_superconductivity}
U.S. Department of Energy, ``DOE Explains... Superconductivity,'' 
\emph{Office of Science}, 2021. [Online]. 
Available: https://www.energy.gov/science/doe-explainssuperconductivity.

\end{thebibliography}
\end{document}


