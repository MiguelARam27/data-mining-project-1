---
title: "Predicting Superconducting Critical Temperature with Linear Regression"
author: "Miguel Ramirez"
date: "September 26, 2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r}
library(glmnet)
library(readr)
library(dplyr)
library(ggplot2)
library(corrplot)
library(caret)
library(Metrics)
library(tidyverse)

set.seed(123)
```

# Data 
```{r}
train = read_csv("train.csv")
unique_m = read_csv("unique_m.csv")

data = na.omit(train)
```


```{r}
X = as.matrix(data[, -which(names(data) == "critical_temp")])
y = data$critical_temp
X_scaled = scale(X)
```

## visualizing TC


```{r histogram-critical-temp}
ggplot(data, aes(x = critical_temp)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "black", alpha = 0.75) +
  labs(title = "Distribution of Critical Temperature ",
       x = "Critical Temperature",
       y = "Count") +
  theme_minimal()
```

## Summary of Temp

```{r}
summary_stats = data %>%
  summarise(
    Min = min(critical_temp, na.rm = TRUE),
    `1st Qu.` = quantile(critical_temp, 0.25, na.rm = TRUE),
    Median = median(critical_temp, na.rm = TRUE),
    Mean = mean(critical_temp, na.rm = TRUE),
    `3rd Qu.` = quantile(critical_temp, 0.75, na.rm = TRUE),
    Max = max(critical_temp, na.rm = TRUE),
    Std = sd(critical_temp, na.rm = TRUE)
  )

knitr::kable(summary_stats, digits = 4, caption = "Five-Number Summary and Standard Deviation of Critical Temperature")

```
```{r coorplot}
corrplot(cor(X), method = "color", type = "upper", tl.cex = 0.7, number.cex = 0.7, title = "Correlation Predictor variables", mar = c(0,0,1,0))
```
Unreadable... so I will do a top 10 table of correlations

```{r multicollinearity}
# Correlation matrix among predictors
cormat = cor(X, use = "pairwise.complete.obs")

# Function to extract top correlated pairs
get_top_pairs = function(cmat, n = 10) {
  m = abs(cmat)
  m[lower.tri(m, diag = TRUE)] = NA
  df = as.data.frame(as.table(m))
  df %>%
    dplyr::filter(!is.na(Freq)) %>%
    arrange(desc(Freq)) %>%
    head(n) %>%
    rename(var1 = Var1, var2 = Var2, abs_corr = Freq)
}

# Top 10 correlated predictor pairs
top_pairs = get_top_pairs(cormat, n = 10)

knitr::kable(
  top_pairs,
  digits = 3,
  caption = "Top 10 Most Correlated Predictor Pairs (|r|)"
)

```
## LASSO Regression

```{r lasso-regression}
lasso_cv = cv.glmnet(X_scaled, y, alpha = 1)
lasso_best_lambda = lasso_cv$lambda.min
lasso_coef = coef(lasso_cv, s = "lambda.min")

lasso_df = as.data.frame(as.matrix(lasso_coef))
lasso_df$Feature = rownames(lasso_df)
colnames(lasso_df)[1] = "Coefficient"
lasso_df = lasso_df %>% dplyr::filter(Coefficient != 0 & Feature != "(Intercept)")
lasso_vars = lasso_df$Feature
lasso_df
```


---

## Ridge Regression

```{r ridge-regression}


ridge_cv = cv.glmnet(X_scaled, y, alpha = 0)
ridge_best_lambda = ridge_cv$lambda.min
ridge_coef = coef(ridge_cv, s = "lambda.min")

ridge_df = as.data.frame(as.matrix(ridge_coef))
ridge_df$Feature = rownames(ridge_df)
colnames(ridge_df)[1] = "Coefficient"

# Keep only predictors with |coef| >= 0.05
ridge_filtered = ridge_df %>%
  dplyr::filter(Feature != "(Intercept)", abs(Coefficient) >= 0.05)


ridge_vars = ridge_filtered$Feature

ridge_filtered %>%
  arrange(desc(abs(Coefficient))) %>%
  knitr::kable(digits = 4, caption = "Ridge-Selected Features (|Coefficient| ≥ 0.05)")

```


---

## Compare Model Accuracy

```{r compare-models-k-fold}
get_cv_metrics = function(var_names, alpha_val) {
  df_subset = data %>% dplyr::select(all_of(var_names))
  X_sub = scale(as.matrix(df_subset))
  y_sub = data$critical_temp
  
  folds = createFolds(y_sub, k = 10)
  rmse_vals = numeric(10)
  mae_vals = numeric(10)
  r2_vals = numeric(10)
  
  for (i in 1:10) {
    test_idx = folds[[i]]
    train_idx = setdiff(1:nrow(X_sub), test_idx)
    model_cv = cv.glmnet(X_sub[train_idx, ], y_sub[train_idx], alpha = alpha_val)
    pred = predict(model_cv, s = "lambda.min", newx = X_sub[test_idx, ])
    rmse_vals[i] = rmse(y_sub[test_idx], pred)
    mae_vals[i] = mae(y_sub[test_idx], pred)
    r2_vals[i] = R2(pred, y_sub[test_idx])
  }
  
  tibble(
    RMSE = mean(rmse_vals),
    MAE = mean(mae_vals),
    R2 = mean(r2_vals)
  )
}

lasso_top = get_cv_metrics(lasso_vars, alpha_val = 1)
ridge_top = get_cv_metrics(ridge_vars, alpha_val = 0)

cv_table = bind_rows(
  lasso_top %>% mutate(Model = "LASSO (non-zero)"),
  ridge_top %>% mutate(Model = "Ridge (|coef| ≥ 0.05)")
) %>%
  dplyr::select(Model, RMSE, MAE, R2)

knitr::kable(cv_table, digits = 4, caption = "10-Fold Cross-Validation: LASSO vs Ridge")
```

---

## Feature Counts

```{r feature-counts}
tibble(
  Model = c("LASSO", "Ridge"),
  `# of Selected Features` = c(length(lasso_vars), length(ridge_vars))
) %>%
  knitr::kable(caption = "Number of Features Selected by Each Model")
```

---

## Residual and Prediction Analysis

```{r model-prediction}
lm_lasso = lm(critical_temp ~ ., data = data %>% dplyr::select(all_of(lasso_vars), critical_temp))
lm_ridge = lm(critical_temp ~ ., data = data %>% dplyr::select(all_of(ridge_vars), critical_temp))

data$pred_lasso = predict(lm_lasso)
data$pred_ridge = predict(lm_ridge)

data$resid_lasso = data$critical_temp - data$pred_lasso
data$resid_ridge = data$critical_temp - data$pred_ridge

blue_trans = adjustcolor("blue", alpha.f = 0.6)
red_trans = adjustcolor("red", alpha.f = 0.6)

par(mfrow = c(1, 2))

plot(data$critical_temp, data$pred_lasso,
     main = "LASSO: Actual vs Predicted",
     xlab = "Actual", ylab = "Predicted", pch = 19, col = blue_trans)
abline(0, 1, col = "black")

plot(data$critical_temp, data$pred_ridge,
     main = "Ridge: Actual vs Predicted",
     xlab = "Actual", ylab = "Predicted", pch = 19, col = red_trans)
abline(0, 1, col = "black")
```

---

## Q-Q Plots for Residual Normality

```{r qq-plots}
par(mfrow = c(1, 2))

qqnorm(data$resid_lasso, main = "Q-Q Plot: LASSO Residuals")
qqline(data$resid_lasso, col = "red")

qqnorm(data$resid_ridge, main = "Q-Q Plot: Ridge Residuals")
qqline(data$resid_ridge, col = "red")
```
